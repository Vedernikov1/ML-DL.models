{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DomAd.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMHY5nMFcn+1SrxpMEj64wA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vedernikov1/ML-DL.models/blob/main/DomAd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Domain shift\n",
        "Целью данного блокнота является проверить данные train и test выборки на доменное смещение, то есть понять, отличаются ли у нас даннные на которых мы обучаемся и данные, на которых нашей моделе придется работать\n",
        "\n",
        "Чтобы понять это, мы для начала получим эмбеддинги предложений из двух наборов данных, а потом обучим классификатор, который должен будет определять принадлежность к тому или другому набору данных. Если классификатор будет обладать низкой точностью (В идеале, в районе 50%, значит между данными нет смещения)"
      ],
      "metadata": {
        "id": "9CX6wlOKZhGc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9aB9cDvDDCh"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Библиотека с предобученными трансформерами\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Для разбиения train выборки\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Библиотеки для классификации\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "# Для подсчета метрик\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score"
      ],
      "metadata": {
        "id": "mGDUzuGTDxkl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Константы\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "TRAIN_FILENAME = 'train.tsv'\n",
        "TEST_FILENAME = 'test.tsv'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "wK2k4GHFDx_x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Создание датасета для классификации"
      ],
      "metadata": {
        "id": "i8H-6SFLaput"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dataset(filename):\n",
        "  df = pd.read_csv(filename, sep=\"\\t\")\n",
        "  return np.array(df['title'])"
      ],
      "metadata": {
        "id": "T2K_-ZzgD4uV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = read_dataset(TRAIN_FILENAME)[:1000] # len = 1000 Чтобы сделать классы сбалансированными\n",
        "X_test = read_dataset(TEST_FILENAME) # len = 1000\n",
        "\n",
        "y_train = np.ones(len(X_train)) # ones - первый класс\n",
        "y_test = np.zeros(len(X_test)) # zeros - второй класс\n",
        "\n",
        "X_data = np.hstack([X_train, X_test])\n",
        "y_data = np.hstack([y_train, y_test])"
      ],
      "metadata": {
        "id": "7RzsX0_qEb8C"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделим полученный датасет на train и test и перемешаем\n",
        "X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "ESyoq6-rL4lH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Токенизация данных и получение эмбеддингов"
      ],
      "metadata": {
        "id": "ZjPeULPeawSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\")\n",
        "\n",
        "tokenized_train = tokenizer(list(X_class_train), padding=True, truncation=True, max_length=24, return_tensors='pt')\n",
        "tokenized_train = tokenized_train.to(DEVICE)\n",
        "\n",
        "tokenized_test = tokenizer(list(X_class_test), padding=True, truncation=True, max_length=24, return_tensors='pt')\n",
        "tokenized_test = tokenized_test.to(DEVICE)"
      ],
      "metadata": {
        "id": "RRAjbvGiPrM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Скачивание предобученной модели\n",
        "model_RuBERT = AutoModel.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\").to(DEVICE)"
      ],
      "metadata": {
        "id": "EAE0UYXRRiGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Данная функция взята с https://huggingface.co '''\n",
        "\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask"
      ],
      "metadata": {
        "id": "56ihXzD-ST0d"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    model_output_train = model_RuBERT(**tokenized_train)\n",
        "with torch.no_grad():\n",
        "    model_output_test = model_RuBERT(**tokenized_test)\n",
        "\n",
        "train_embeddings = mean_pooling(model_output_train, tokenized_train['attention_mask'])\n",
        "test_embeddings = mean_pooling(model_output_test, tokenized_test['attention_mask'])"
      ],
      "metadata": {
        "id": "z2WrOYYeSUg3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение классификационных моделей"
      ],
      "metadata": {
        "id": "LdNTMFmRa7oW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classification_models = dict()\n",
        "\n",
        "clf = make_pipeline(StandardScaler(), SVC(kernel='rbf', gamma='scale'))\n",
        "clf.fit(train_embeddings.cpu(), y_class_train)\n",
        "classification_models['SVM'] = clf\n",
        "\n",
        "clf = make_pipeline(StandardScaler(), RandomForestClassifier())\n",
        "clf.fit(train_embeddings.cpu(), y_class_train)\n",
        "classification_models['RandomForestClassifier'] = clf\n",
        "\n",
        "clf = make_pipeline(StandardScaler(), LogisticRegression())\n",
        "clf.fit(train_embeddings.cpu(), y_class_train)\n",
        "classification_models['LogisticRegeression'] = clf\n",
        "\n",
        "clf = make_pipeline(StandardScaler(), xgb.XGBClassifier())\n",
        "clf.fit(train_embeddings.cpu(), y_class_train)\n",
        "classification_models['GBM'] = clf"
      ],
      "metadata": {
        "id": "lz_DgDsLSW-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Подсчет метрик"
      ],
      "metadata": {
        "id": "DXlHjlVbbBBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(y_true, y_pred):\n",
        "  metric_scores = dict()\n",
        "\n",
        "  metric_scores['f1_score'] = f1_score(y_true, y_pred)\n",
        "  metric_scores['roc-auc'] = roc_auc_score(y_true, y_pred)\n",
        "  metric_scores['accuracy'] = accuracy_score(y_true, y_pred)\n",
        "\n",
        "  return metric_scores"
      ],
      "metadata": {
        "id": "wdDh7NpkScVZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, class_model in classification_models.items():\n",
        "  y_pred = np.array(class_model.predict(test_embeddings.cpu()))\n",
        "  metrics = compute_metrics(y_class_test, y_pred)\n",
        "\n",
        "  print(f'{model_name}:')\n",
        "  for score_name, score in metrics.items():\n",
        "    print(f'{score_name}: {score}')\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1Qsr8joSecU",
        "outputId": "0acc5de4-bb19-4027-8e1d-9c08d42d6342"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM:\n",
            "f1_score: 0.4533333333333333\n",
            "roc-auc: 0.48719967999199976\n",
            "accuracy: 0.4875\n",
            "\n",
            "RandomForestClassifier:\n",
            "f1_score: 0.47120418848167533\n",
            "roc-auc: 0.4947873696842421\n",
            "accuracy: 0.495\n",
            "\n",
            "LogisticRegeression:\n",
            "f1_score: 0.4421052631578947\n",
            "roc-auc: 0.4697617440436011\n",
            "accuracy: 0.47\n",
            "\n",
            "GBM:\n",
            "f1_score: 0.529262086513995\n",
            "roc-auc: 0.5374259356483913\n",
            "accuracy: 0.5375\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Анализ и интерпретируемость\n",
        "\n",
        "Мы получили качество всех классификаторов в районе 50%, а это в свою очередь означает отсутствие доменного смещения и можно обучать основную модель без доменной адаптации"
      ],
      "metadata": {
        "id": "KkIIRYBZbKga"
      }
    }
  ]
}